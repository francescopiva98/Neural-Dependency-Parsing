Project Overview
This project explores transitionâ€‘based dependency parsing using a modified version of the arcâ€‘eager algorithm with a static oracle, a widely adopted syntactic representation in natural language processing thanks to its balance between linguistic expressivity, annotation cost, and computational efficiency.

The goal is to investigate how different neural architectures influence parsing performance when integrated into a transitionâ€‘based framework.

ðŸ”§ Methodology
The work is structured into several key components:
- Arcâ€‘eager algorithm with static oracle: formal definition and handling of edge cases where the stack or buffer contain a single element.
- Baseline neural parser: a biâ€‘LSTM encoder extracts contextual features from input tokens.
- BERTâ€‘based parser: replacement of the biâ€‘LSTM with transformerâ€‘based contextual embeddings, comparing Multilingual Cased BERT and Latinâ€‘BERT.
- Model comparison: evaluation against stateâ€‘ofâ€‘theâ€‘art parsers under identical training conditions.
- Conclusions: analysis of strengths, limitations, and future directions.

In both architectures, the next transition is predicted using an MLP fed with the top elements of the stack and the first element of the buffer. Since arcâ€‘eager configurations may lack one of these components, special cases are explicitly handled.

ðŸ“Š Evaluation
Arc labels are discarded, and performance is measured using Unlabelled Attachment Score (UAS) to ensure a fair comparison across models and languages.

ðŸŽ¯ Repository Purpose
This repository was developed as part of a university NLP course, with the aim of experimenting with neural transitionâ€‘based dependency parsing and assessing the impact of contextual embeddings on parsing accuracy.